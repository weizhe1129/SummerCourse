{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"images/book_cover.jpg\" width=\"120\">\n",
    "\n",
    "*This notebook contains an excerpt from the [Python Programming and Numerical Methods - A Guide for Engineers and Scientists](https://www.elsevier.com/books/python-programming-and-numerical-methods/kong/978-0-12-819549-9), the content is also available at [Berkeley Python Numerical Methods](https://pythonnumericalmethods.berkeley.edu/notebooks/Index.html).*\n",
    "\n",
    "*The copyright of the book belongs to Elsevier. We also have this interactive book online for a better learning experience. The code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work on [Elsevier](https://www.elsevier.com/books/python-programming-and-numerical-methods/kong/978-0-12-819549-9) or [Amazon](https://www.amazon.com/Python-Programming-Numerical-Methods-Scientists/dp/0128195495/ref=sr_1_1?dchild=1&keywords=Python+Programming+and+Numerical+Methods+-+A+Guide+for+Engineers+and+Scientists&qid=1604761352&sr=8-1)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "< [14.5 Matrix Inversion](chapter14.05-Matrix-Inversion.ipynb)  | [Contents](Index.ipynb) | [CHAPTER 15.  Eigenvalues and Eigenvectors](chapter15.00-Eigenvalues-and-Eigenvectors.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "1. Linear algebra is the foundation of many engineering fields.\n",
    "2. Vectors can be considered as points in ${\\mathbb{R}}^n$; addition and multiplication are defined on them, although not necessarily the same as for scalars.\n",
    "3. A set of vectors is linearly independent if none of the vectors can be written as a linear combination of the others.\n",
    "4. Matrices are tables of numbers. They have several important properties including the determinant, rank, and inverse.\n",
    "5. A system of linear equations can be represented by the matrix equation $Ax = y$.\n",
    "6. The number of solutions to a system of linear equations is related to the rank($A$) and the rank($[A,y]$). It can be zero, one, or infinity.\n",
    "7. We can solve the equations using Gauss Elimination, Gauss-Jordan Elimination, LU decomposition and Gauss-Seidel method. \n",
    "8. We introduced methods to find matrix inversion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Problems\n",
    "\n",
    "1. Show that matrix multiplication distributes over matrix addition: show $A(B + C) = AB + AC$ assuming that $A, B$, and $C$ are matrices of compatible size.\n",
    "\n",
    "2. Write a function *my_is_orthogonal(v1,v2, tol)*, where $v1$ and $v2$ are column vectors of the same size and $tol$ is a scalar value strictly larger than 0. The output should be 1 if the angle between $v1$ and $v2$ is within tol of $\\pi/2$; that is, $|\\pi/2 - \\theta| < \\text{tol}$, and 0 otherwise. You may assume that $v1$ and $v2$ are column vectors of the same size, and that $tol$ is a positive scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cases for problem 2\n",
    "a = np.array([[1], [0.001]])\n",
    "b = np.array([[0.001], [1]])\n",
    "# output: 1\n",
    "my_is_orthogonal(a,b, 0.01)\n",
    "\n",
    "# output: 0\n",
    "my_is_orthogonal(a,b, 0.001)\n",
    "\n",
    "# output: 0\n",
    "a = np.array([[1], [0.001]])\n",
    "b = np.array([[1], [1]])\n",
    "my_is_orthogonal(a,b, 0.01)\n",
    "\n",
    "# output: 1\n",
    "a = np.array([[1], [1]])\n",
    "b = np.array([[-1], [1]])\n",
    "my_is_orthogonal(a,b, 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. Write a function *my_is_similar(s1,s2,tol)* where $s1$ and $s2$ are strings, not necessarily the same size, and $tol$ is a scalar value strictly larger than 0. From $s1$ and $s2$, the function should construct two vectors, $v1$ and $v2$, where $v1[0]$ is the number of 'a's in $s1$, $v1[1]$ is the number 'b's in $s1$, and so on until $v1[25]$, which is the number of 'z's in $v1$. The vector $v2$ should be similarly constructed from $s2$. The output should be 1 if the absolute value of the angle between $v1$ and $v2$ is less than tol; that is, $|\\theta| < \\text{tol}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "4. Write a function *my_make_lin_ind(A)*, where ${A}$ and ${B}$ are matrices. Let the ${rank(A) = n}$. Then ${B}$ should be a matrix containing the first $n$ columns of ${A}$ that are all linearly independent. Note that this implies that ${B}$ is full rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## Test cases for problem 4\n",
    "\n",
    "A = np.array([[12,24,0,11,-24,18,15], \n",
    "              [19,38,0,10,-31,25,9], \n",
    "              [1,2,0,21,-5,3,20],\n",
    "              [6,12,0,13,-10,8,5],\n",
    "              [22,44,0,2,-12,17,23]])\n",
    "\n",
    "B = my_make_lin_ind(A)\n",
    "\n",
    "# B = [[12,11,-24,15],\n",
    "#      [19,10,-31,9],\n",
    "#      [1,21,-5,20],\n",
    "#      [6,13,-10,5],\n",
    "#      [22,2,-12,23]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "5. Cramer's rule is a method of computing the determinant of a matrix. Consider an ${n} \\times {n}$ square matrix $M$. Let $M(i,j)$ be the element of $M$ in the $i$-th row and $j$-th column of $M$, and let $m_{i,j}$ be the minor of $M$ created by removing the $i$-th row and $j$-th column from $M$. Cramer's rule says that\n",
    "$$\n",
    "\\text{det(M)} = \\sum_{i = 1}^{n} (-1)^{i-1} M(1,i) \\text{det}(m_{i,j}).\n",
    "$$\n",
    "\n",
    " Write a function *my_rec_det(M)*, where the output is $det(M)$. The function should use Cramer's rule to compute the determinant, not Numpy's function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "6. What is the complexity of *my_rec_det* in the previous problem? Do you think this is an effective way of determining if a matrix is singular or not?\n",
    "\n",
    "7. Let ${p}$ be a vector with length ${L}$ containing the coefficients of a polynomial of order ${L-1}$. For example, the vector ${p = [1,0,2]}$ is a representation of the polynomial $f(x) = 1 x^2 + 0 x + 2$. Write a function *my_poly_der_mat(p)*, where ${p}$ is the aforementioned vector, and the output $D$ is the matrix that will return the coefficients of the derivative of ${p}$ when ${p}$ is left multiplied by ${D}$. For example, the derivative of $f(x)$ is $f'(x) = 2x$, and therefore, $d = Dp$ should yield ${d = [2,0]}$. Note this implies that the dimension of $D$ is ${L-1} \\times {L}$. The point of this problem is to show that integrating polynomials is actually a linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "8. Use Gauss Elimination to solve the following equations. \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "3x_1 - x_2 + 4x_3 &=& 2 \\\\\n",
    "17x_1 + 2x_2 + x_3 &=& 14 \\\\\n",
    "x_1 + 12x_2 -7z &=& 54 \\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "9. Use Gauss-Jordan Elimination to solve the above equations in problem 8.\n",
    "\n",
    "10. Get the lower triangular matrix $L$ and upper triangular matrix $U$ from the equations in problem 8.  \n",
    "\n",
    "11. Show that the dot product distributes across vector addition, that is, show that $u \\cdot (v + w) = u \\cdot v + u \\cdot w$. \n",
    "\n",
    "12. Consider the following network consisting of two power supply stations denoted by $S1$ and $S2$ and five power recipient nodes denoted by $N1$ to $N5$. The nodes are connected by power lines, which are denoted by arrows, and power can flow between nodes along these lines in both directions.\n",
    "\n",
    " Let $d_{i}$ be a positive scalar denoting the power demands for node $i$, and assume that this demand must be met exactly. The capacity of the power supply stations is denoted by $S$. Power supply stations must run at their capacity. For each arrow, let $f_{j}$ be the power flow along that arrow. Negative flow implies that power is running in the opposite direction of the arrow.\n",
    " \n",
    " <img src=\"./images/14.01.01-problem_11.png\" alt=\"Problem 11\" style=\"width: 200px;\"/>\n",
    "\n",
    " Write a function *my_flow_calculator(S, d)*, where $S$ is a $1 \\times 2$ vector representing the capacity of each power supply station, and $d$ is a $1 \\times 5$ row vector representing the demands at each node (i.e., $d[0]$ is the demand at node 1). The output argument, $f$, should be a $1 \\times 7$ row vector denoting the flows in the network (i.e., $f[0] = f_1$ in the diagram). The flows contained in $f$ should satisfy all constraints of the system, like power generation and demands. Note that there may be more than one solution to the system of equations.\n",
    " \n",
    " The total flow into a node must equal the total flow out of the node plus the demand; that is, for each node $i, f_{\\text{inflow}} = f_{\\text{outflow}} + d_{i}$. You may assume that $\\Sigma{S_j} = \\Sigma{d_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## Test cases for problem 4\n",
    "\n",
    "s = np.array([[10, 10]])\n",
    "d = np.array([[4, 4, 4, 4, 4]])\n",
    "\n",
    "# f = [[10.0, 4.0, -2.0, 4.5, 5.5, 2.5, 1.5]]\n",
    "f = my_flow_calculator(s, d)\n",
    "\n",
    "s = np.array([[10, 10]])\n",
    "d = np.array([[3, 4, 5, 4, 4]])\n",
    "# f = [[10.0, 5.0, -1.0, 4.5, 5.5, 2.5, 1.5]]\n",
    "f = my_flow_calculator(s, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "< [14.5 Matrix Inversion](chapter14.05-Matrix-Inversion.ipynb)  | [Contents](Index.ipynb) | [CHAPTER 15.  Eigenvalues and Eigenvectors](chapter15.00-Eigenvalues-and-Eigenvectors.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
